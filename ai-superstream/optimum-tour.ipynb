{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5512190-6089-45f6-999f-0f3cd0c07548",
   "metadata": {},
   "source": [
    "# Accelerating Transformers with Hugging Face Optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b808e27-e3b0-4a36-bd44-cdaa295e2d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a8eac-598a-4c5c-afad-096ee26d7373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53a2dfb-2389-443f-a012-9000cff8053e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39685151-312e-47c6-aedb-7bfa1bc8953e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1ad2673-a8db-4611-a88f-7154493c679c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc150082881244b4a7afe7aec8af3fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4610fba6aae5436b8eac9e85ec19f013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/473M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9041663408279419, 'start': 11, 'end': 18, 'answer': 'Philipp'}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "\n",
    "model_id = \"deepset/roberta-base-squad2\"\n",
    "onnx_path = Path(\"onnx\")\n",
    "task = \"question-answering\"\n",
    "\n",
    "# load vanilla transformers and convert to onnx\n",
    "model = ORTModelForQuestionAnswering.from_pretrained(model_id, from_transformers=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# save onnx checkpoint and tokenizer\n",
    "model.save_pretrained(onnx_path)\n",
    "tokenizer.save_pretrained(onnx_path)\n",
    "\n",
    "# test the model with using transformers pipeline, with handle_impossible_answer for squad_v2 \n",
    "optimum_qa = pipeline(task, model=model, tokenizer=tokenizer, handle_impossible_answer=True)\n",
    "prediction = optimum_qa(question=\"What's my name?\", context=\"My name is Philipp and I live in Nuremberg.\")\n",
    "\n",
    "print(prediction)\n",
    "# {'score': 0.9041663408279419, 'start': 11, 'end': 18, 'answer': 'Philipp'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db95826-7ec4-4360-801e-efe07844b330",
   "metadata": {},
   "source": [
    "## Optimising the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a97e4383-93ab-4ae5-a0e2-42d6eeebcf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 09:56:50.393184481 [W:onnxruntime:, inference_session.cc:1546 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('onnx/model-optimized.onnx')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTOptimizer\n",
    "from optimum.onnxruntime.configuration import OptimizationConfig\n",
    "\n",
    "# create ORTOptimizer and define optimization configuration\n",
    "optimizer = ORTOptimizer.from_pretrained(model_id, feature=task)\n",
    "optimization_config = OptimizationConfig(optimization_level=99) # enable all optimizations\n",
    "\n",
    "# apply the optimization configuration to the model\n",
    "optimizer.export(\n",
    "    onnx_model_path=onnx_path / \"model.onnx\",\n",
    "    onnx_optimized_model_output_path=onnx_path / \"model-optimized.onnx\",\n",
    "    optimization_config=optimization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b436e2ab-859e-46ba-b08f-afa23e0a8b97",
   "metadata": {},
   "source": [
    "## Quantizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e97128c7-8e81-4fcb-abb2-7e19d1742a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('onnx/model-quantized.onnx')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "\n",
    "# create ORTQuantizer and define quantization configuration\n",
    "quantizer = ORTQuantizer.from_pretrained(model_id, feature=task)\n",
    "qconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=True)\n",
    "\n",
    "# apply the quantization configuration to the model\n",
    "quantizer.export(\n",
    "    onnx_model_path=onnx_path / \"model-optimized.onnx\",\n",
    "    onnx_quantized_model_output_path=onnx_path / \"model-quantized.onnx\",\n",
    "    quantization_config=qconfig,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da73add3-5185-48a7-bb5e-5b2c9bf8ea3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla Onnx Model file size: 473.34 MB\n",
      "Quantized Onnx Model file size: 291.30 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# get model file size\n",
    "size = os.path.getsize(onnx_path / \"model.onnx\")/(1024*1024)\n",
    "print(f\"Vanilla Onnx Model file size: {size:.2f} MB\")\n",
    "size = os.path.getsize(onnx_path / \"model-quantized.onnx\")/(1024*1024)\n",
    "print(f\"Quantized Onnx Model file size: {size:.2f} MB\")\n",
    "\n",
    "# Vanilla Onnx Model file size: 473.31 MB\n",
    "# Quantized Onnx Model file size: 291.77 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bd5e852-652e-4c38-b08a-af72b59cf6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9032881259918213, 'start': 11, 'end': 18, 'answer': 'Philipp'}\n"
     ]
    }
   ],
   "source": [
    "# load quantized model\n",
    "quantized_model = ORTModelForQuestionAnswering.from_pretrained(onnx_path, file_name=\"model-quantized.onnx\")\n",
    "\n",
    "# test the quantized model with using transformers pipeline\n",
    "quantized_optimum_qa = pipeline(task, model=quantized_model, tokenizer=tokenizer, handle_impossible_answer=True)\n",
    "prediction = quantized_optimum_qa(question=\"What's my name?\", context=\"My name is Philipp and I live in Nuremberg.\")\n",
    "print(prediction)\n",
    "# {'score': 0.9246969819068909, 'start': 11, 'end': 18, 'answer': 'Philipp'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914b3576-d282-4e91-b1bc-819c6c67dc7f",
   "metadata": {},
   "source": [
    "## Run inference with Transformers pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e74c254-2238-4f59-9d46-a1dd31e998e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9041663408279419, 'start': 11, 'end': 18, 'answer': 'Philipp'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(onnx_path)\n",
    "model = ORTModelForQuestionAnswering.from_pretrained(onnx_path)\n",
    "\n",
    "optimum_qa = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "prediction = optimum_qa(question=\"What's my name?\", context=\"My name is Philipp and I live in Nuremberg.\")\n",
    "\n",
    "print(prediction)\n",
    "# {'score': 0.9041663408279419, 'start': 11, 'end': 18, 'answer': 'Philipp'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50f673f1-ccaf-49bf-b07c-ad8f22328735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9041663408279419, 'start': 11, 'end': 18, 'answer': 'Philipp'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "from optimum.pipelines import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(onnx_path)\n",
    "model = ORTModelForQuestionAnswering.from_pretrained(onnx_path)\n",
    "\n",
    "optimum_qa = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, handle_impossible_answer=True)\n",
    "prediction = optimum_qa(question=\"What's my name?\", context=\"My name is Philipp and I live in Nuremberg.\")\n",
    "\n",
    "print(prediction)\n",
    "# {'score': 0.9041663408279419, 'start': 11, 'end': 18, 'answer': 'Philipp'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bb508d-0260-436f-b76f-0326bfd49d36",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dea108a-a3a7-4183-9a42-a3ac8d497359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe40fe2c44a432a812103007e5d949c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5b1c1afdd24ab9946cc605c4de8232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90ae990342c47209a7cb6b66c2ba594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.87k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99b4e668c304bb7bf477d6d2c208b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset squad_v2/squad_v2 (download: 44.34 MiB, generated: 122.41 MiB, post-processed: Unknown size, total: 166.75 MiB) to /home/lewis/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192c69a08e6f46c2bc136797fa23f71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7bccc92556b40d3864bfcf72e44fb50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/9.55M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f065f085d504cef8416dd130b7156fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/801k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef52f7179044e25b3e95ad13fb2d527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/130319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset squad_v2 downloaded and prepared to /home/lewis/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc983daec04a431e9161769aedca783e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset 11873\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric,load_dataset\n",
    "\n",
    "metric = load_metric(\"squad_v2\")\n",
    "dataset = load_dataset(\"squad_v2\")[\"validation\"]\n",
    "\n",
    "print(f\"length of dataset {len(dataset)}\")\n",
    "#length of dataset 11873"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06bd06b1-c4d4-4cd6-aa71-b649890b4a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f497df83ce646018b8611ddb155bc94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d49c5d49666413ba43c331d9dc57784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01646d63a03d40f29289c527d953e1eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527a7d6f510e486296c0e73b8884d1cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate(example):\n",
    "    default = optimum_qa(question=example[\"question\"], context=example[\"context\"])\n",
    "    quantized = quantized_optimum_qa(question=example[\"question\"], context=example[\"context\"])\n",
    "    return {\n",
    "      'reference': {'id': example['id'], 'answers': example['answers']},\n",
    "      'default': {'id': example['id'],'prediction_text': default['answer'], 'no_answer_probability': 0.},\n",
    "      'quantized': {'id': example['id'],'prediction_text': quantized['answer'], 'no_answer_probability': 0.},\n",
    "      }\n",
    "\n",
    "# result = dataset.map(evaluate)\n",
    "# COMMENT IN to run evaluation on 2000 subset of the dataset\n",
    "result = dataset.shuffle().select(range(2000)).map(evaluate, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2112162f-b2a8-45e9-bc2b-877fc87b1f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers', 'reference', 'default', 'quantized'],\n",
       "    num_rows: 2000\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8ed6196-31ad-4294-924c-a27be517c490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model: exact=79.5% f1=83.06556122280429%\n",
      "quantized model: exact=78.35% f1=81.80711697925774%\n"
     ]
    }
   ],
   "source": [
    "default_acc = metric.compute(predictions=result[\"default\"], references=result[\"reference\"])\n",
    "quantized = metric.compute(predictions=result[\"quantized\"], references=result[\"reference\"])\n",
    "\n",
    "print(f\"vanilla model: exact={default_acc['exact']}% f1={default_acc['f1']}%\")\n",
    "print(f\"quantized model: exact={quantized['exact']}% f1={quantized['f1']}%\")\n",
    "\n",
    "# vanilla model: exact=79.07858165585783% f1=82.14970024570314%\n",
    "# optimized model: exact=79.07858165585783% f1=82.14970024570314%\n",
    "# quantized model: exact=78.75010528088941% f1=81.82526107204629%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52078834-14ce-42c3-8b3a-ad42062d13fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "context=\"Hello, my name is Philipp and I live in Nuremberg, Germany. Currently I am working as a Technical Lead at Hugging Face to democratize artificial intelligence through open source and open science. In the past I designed and implemented cloud-native machine learning architectures for fin-tech and insurance companies. I found my passion for cloud concepts and machine learning 5 years ago. Since then I never stopped learning. Currently, I am focusing myself in the area NLP and how to leverage models like BERT, Roberta, T5, ViT, and GPT2 to generate business value.\" \n",
    "question=\"As what is Philipp working?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4ad12fb-5316-48a7-9292-16773ffa55fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla model Average latency (ms) - 38.25 +\\- 0.32\n",
      "Optimized & Quantized model Average latency (ms) - 26.34 +\\- 0.06\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "import numpy as np \n",
    "\n",
    "def measure_latency(pipe):\n",
    "    latencies = []\n",
    "    # warm up\n",
    "    for _ in range(10):\n",
    "        _ = pipe(question=question, context=context)\n",
    "    # Timed run\n",
    "    for _ in range(100):\n",
    "        start_time = perf_counter()\n",
    "        _ =  pipe(question=question, context=context)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    # Compute run statistics\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    return f\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\"\n",
    "\n",
    "print(f\"Vanilla model {measure_latency(optimum_qa)}\")\n",
    "print(f\"Optimized & Quantized model {measure_latency(quantized_optimum_qa)}\")\n",
    "\n",
    "# Vanilla model Average latency (ms) - 117.61 +\\- 8.48\n",
    "# Optimized & Quantized model Average latency (ms) - 64.94 +\\- 3.65\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cde837b-9fb8-49ea-8e80-1329d275fccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
