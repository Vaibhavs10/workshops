{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "628d20db-4bb6-49bd-a43b-72897b831826",
   "metadata": {},
   "source": [
    "{{ badge }}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee47bc31-6686-4cf4-b81d-1ac6b0a99a05",
   "metadata": {},
   "source": [
    "# Dynamic Quantization with Hugging Face Optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c100e377-0f26-4966-a76c-d4e1f5dfe3de",
   "metadata": {},
   "source": [
    "In this session, you will learn how to apply _dynamic quantization_ to a ðŸ¤— Transformers model. You will quantize a [DistilBERT model](https://huggingface.co/optimum/distilbert-base-uncased-finetuned-banking77) that's been fine-tuned on the [Banking77 dataset](https://huggingface.co/datasets/banking77) for intent classification. \n",
    "\n",
    "Along the way, you'll learn how to use two open-source libraries: \n",
    "\n",
    "* [ðŸ¤— Optimum](https://github.com/huggingface/optimum): an extension of ðŸ¤— Transformers, which provides a set of performance optimization tools enabling maximum efficiency to train and run models on targeted hardware.\n",
    "* [ðŸ¤— Evaluate](https://github.com/huggingface/evaluate): a library that makes evaluating and comparing models and reporting their performance easier and more standardized.\n",
    "\n",
    "\n",
    "By the end of this session, you see how quantization with ðŸ¤— Optimum can significantly decrease model latency while keeping almost 100% of the full-precision model.\n",
    "\n",
    "\n",
    "> This tutorial was created and run on a c6i.xlarge AWS EC2 Instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4149dbcf-508c-4c70-b794-91376a7e3662",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "By the end of this session, you will know how to:\n",
    "\n",
    "* Setup a development environment\n",
    "* Convert a ðŸ¤— Transformers model to ONNX for inference\n",
    "* Apply dynamic quantization using `ORTQuantizer` from ðŸ¤— Optimum\n",
    "* Test inference with the quantized model\n",
    "* Evaluate the model performance with ðŸ¤— Evaluate\n",
    "* Push the quantized model to the Hub\n",
    "* Load and run inference with a quantized model from the Hub\n",
    "\n",
    "\n",
    "Let's get started! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0795d087-77f9-4d82-915f-983f13abf4ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Setup development environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bdd46b-9161-4a09-acfa-cbcd095f55e2",
   "metadata": {},
   "source": [
    "Our first step is to install ðŸ¤— Optimum, along with ðŸ¤— Evaluate and some other libraries. Running the following cell will install all the required packages for us including ðŸ¤— Transformer, PyTorch, and ONNX Runtime utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327dfc3f-02be-4816-bd20-305607accef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"optimum[onnxruntime]==1.2.2\" \"git+https://github.com/huggingface/evaluate.git#egg=evaluate[evaluator]\" sklearn mkl-include mkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90e16fc-e9bb-41f0-aa50-948a3c4c7d25",
   "metadata": {},
   "source": [
    "> If you want to run inference on a GPU, you can install ðŸ¤— Optimum with `pip install optimum[onnxruntime-gpu]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501f2de8-044f-4c99-acad-d9386171b863",
   "metadata": {},
   "source": [
    "The final setup step is to login to our Hugging Face account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8682b6b-21ac-4c1a-ab4f-d7ac602e0dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176f523b75414e118ba978bf7f426355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b84656-0248-4b39-a8cb-65bd3b615a0d",
   "metadata": {},
   "source": [
    "While we're at it, let's disable the parallelism in the tokenizers to avoid a long list of warnings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c8841c5-79c6-4ff7-936f-832faaf706cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a977d3-9006-47a7-aefb-6d5ea79c00f8",
   "metadata": {},
   "source": [
    "## 2. Convert a ðŸ¤— Transformers model to ONNX for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9680fe1b-e176-49ed-8ed6-f0ec35049d2b",
   "metadata": {},
   "source": [
    "Before we can optimize and quantize our model, we first need to export it to the ONNX format. To do this we will use the `ORTModelForSequenceClassification` class and call the `from_pretrained()` method. This method will download the PyTorch weights from the Hub and export them via the `from_transformers` argument. The model we are using is `optimum/distilbert-base-uncased-finetuned-banking77`, which is a fine-tuned DistilBERT model on the Banking77 dataset achieving an accuracy score of 92.5% and as the feature (task) text-classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0866e06f-cadc-412c-9bc4-ab7c77d84e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8edd25bafab84a119ee00c77956c0a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "model_id=\"optimum/distilbert-base-uncased-finetuned-banking77\"\n",
    "dataset_id=\"banking77\"\n",
    "onnx_path = Path(\"onnx\")\n",
    "\n",
    "# load vanilla transformers and convert to onnx\n",
    "model = ORTModelForSequenceClassification.from_pretrained(model_id, from_transformers=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3913851-68c9-45ee-b569-d68c47388ced",
   "metadata": {},
   "source": [
    "One neat thing about ðŸ¤— Optimum, is that allows you to run ONNX models with the `pipeline()` function from  ðŸ¤— Transformers. This means, you get all the pre- and post-processing features for free, without needing to re-implement them for each model! Here's how you can run inference with our vanilla ONNX model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5930fe08-a929-42f5-989c-95c826cbfd6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'lost_or_stolen_card', 'score': 0.9664045572280884}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "vanilla_clf = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "vanilla_clf(\"Could you assist me in finding my lost card?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b143952-97dd-4b5e-b494-439fee87078c",
   "metadata": {},
   "source": [
    "This looks good, so let's save the model and tokenizer to disk for later usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d012c47b-9018-4f28-b6aa-d83d3b011514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('onnx/tokenizer_config.json',\n",
       " 'onnx/special_tokens_map.json',\n",
       " 'onnx/vocab.txt',\n",
       " 'onnx/added_tokens.json',\n",
       " 'onnx/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save onnx checkpoint and tokenizer\n",
    "model.save_pretrained(onnx_path)\n",
    "tokenizer.save_pretrained(onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e9d6dd-b1de-4f7c-92e6-7857c443e29c",
   "metadata": {},
   "source": [
    "If we inspect the `onnx` directory where we've saved the model and tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c647267e-c05a-4f3a-b80f-afe24de6ebfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t\t      special_tokens_map.json  vocab.txt\n",
      "model-quantized-dynamic.onnx  tokenizer.json\n",
      "model.onnx\t\t      tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "!ls {onnx_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663e0138-b942-4d15-a98a-345de6d3b286",
   "metadata": {},
   "source": [
    "we can see that there's a `model.onnx` file that corresponds to our exported model. Let's now go ahead and optimize this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60b2a17-d71b-47a8-b4f3-2e6b5d05dd4d",
   "metadata": {},
   "source": [
    "## 3. Apply dynamic quantization using `ORTQuantizer` from ðŸ¤— Optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c07243-2c83-4443-a217-7e460789bcd1",
   "metadata": {},
   "source": [
    "To apply quantization in ðŸ¤— Optimum, we do this by:\n",
    "\n",
    "* Creating an optimizer based on our ONNX model\n",
    "* Defining the type of optimizations via a configuration class\n",
    "* Exporting the optimized model as a new ONNX file\n",
    "\n",
    "The following code snippet does these steps for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9f5bbe9-bf45-4f4b-8a62-4607bfd89c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "\n",
    "# create ORTQuantizer and define quantization configuration\n",
    "dynamic_quantizer = ORTQuantizer.from_pretrained(model_id, feature=\"sequence-classification\")\n",
    "dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=True)\n",
    "\n",
    "# apply the quantization configuration to the model\n",
    "model_quantized_path = dynamic_quantizer.export(\n",
    "    onnx_model_path=onnx_path / \"model.onnx\",\n",
    "    onnx_quantized_model_output_path=onnx_path / \"model-quantized-dynamic.onnx\",\n",
    "    quantization_config=dqconfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac9ed27-261b-4517-8612-9f5cb3d7fc12",
   "metadata": {},
   "source": [
    "Here we can see that we've specifed in the configuration the type of execution engine to use with the Intel AVX512-VNNI CPU. If we now take a look at our `onnx` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "857de870-66a4-4047-bae0-80ad8ab51512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t\t      special_tokens_map.json  vocab.txt\n",
      "model-quantized-dynamic.onnx  tokenizer.json\n",
      "model.onnx\t\t      tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "!ls {onnx_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bdea9b-1a03-42c4-accf-a0bfcf4b115b",
   "metadata": {},
   "source": [
    "we can see we have a new ONNX file called `model-quantized-dynamic.onnx`. Let's do a quick size comparison of the two models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd67a815-421d-4b0a-a41e-28cdb11ea256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file size: 255.68 MB\n",
      "Quantized Model file size: 134.43 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# get model file size\n",
    "size = os.path.getsize(onnx_path / \"model.onnx\")/(1024*1024)\n",
    "quantized_model = os.path.getsize(onnx_path / \"model-quantized-dynamic.onnx\")/(1024*1024)\n",
    "\n",
    "print(f\"Model file size: {size:.2f} MB\")\n",
    "print(f\"Quantized Model file size: {quantized_model:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff232e15-5baa-4d00-b7bb-93407144889f",
   "metadata": {},
   "source": [
    "Nice, dynamic quantization has reduced the model size by around a factor of 2! This should allow us to speed up the inference time by a similar factor, so let's now see how we can test the latency of our models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181a828c-fc79-403c-b2e0-305a2f0302bb",
   "metadata": {},
   "source": [
    "## 4. Test inference with the quantized model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86af0f26-fbf4-4016-81ff-c822ff6383c1",
   "metadata": {},
   "source": [
    "As we saw earlier, Optimum has built-in support for transformers pipelines. This allows us to leverage the same API that we know from using PyTorch and TensorFlow models. Therefore we can load our quantized model with `ORTModelForSequenceClassification` class and the transformers `pipeline()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2bdc068-d03f-41b8-979b-843759885099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'cash_withdrawal_not_recognised', 'score': 0.06697972863912582}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ORTModelForSequenceClassification.from_pretrained(onnx_path, file_name=\"model-quantized-dynamic.onnx\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(onnx_path)\n",
    "\n",
    "quantized_clf = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "quantized_clf(\"Could you assist me in finding my lost card?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e18775e-3e34-4d94-8643-865fa402e56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default\n",
      "WARNING:datasets.builder:Reusing dataset banking77 (/home/ubuntu/.cache/huggingface/datasets/banking77/default/1.1.0/aec0289529599d4572d76ab00c8944cb84f88410ad0c9e7da26189d31f62a55b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.012987012987012988}\n"
     ]
    }
   ],
   "source": [
    "from evaluate import evaluator\n",
    "from datasets import load_dataset \n",
    "\n",
    "eval_pipe = evaluator(\"text-classification\")\n",
    "eval_dataset = load_dataset(\"banking77\", split=\"test\")\n",
    "label_mapping = model.config.label2id\n",
    "\n",
    "results = eval_pipe.compute(\n",
    "    model_or_pipeline=quantized_clf,\n",
    "    data=eval_dataset,\n",
    "    metric=\"accuracy\",\n",
    "    input_column=\"text\",\n",
    "    label_column=\"label\",\n",
    "    label_mapping=label_mapping,\n",
    "    strategy=\"simple\",\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c076191e-555f-4ac7-ae11-22414ce99cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla model: 92.5%\n",
      "Quantized model: 1.30%\n",
      "The quantized model achieves 1.40% accuracy of the fp32 model\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vanilla model: 92.5%\")\n",
    "print(f\"Quantized model: {results['accuracy']*100:.2f}%\")\n",
    "print(f\"The quantized model achieves {round(results['accuracy']/0.925,4)*100:.2f}% accuracy of the fp32 model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca1d748-0fb5-4d23-9ac4-0a5052d3da79",
   "metadata": {},
   "source": [
    "Okay, now let's test the performance (latency) of our quantized model. We are going to use a payload with a sequence length of 128 for the benchmark. To keep it simple, we are going to use a python loop and calculate the avg,mean & p95 latency for our vanilla model and for the quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db0ecd12-b9a6-44f5-b4e3-3984ff8529e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload sequence length: 128\n",
      "Vanilla model: P95 latency (ms) - 134.45046685028507; Average latency (ms) - 124.96 +\\- 45.30;\n",
      "Quantized model: P95 latency (ms) - 92.54824770014238; Average latency (ms) - 72.11 +\\- 13.55;\n",
      "Improvement through quantization: 1.45x\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "import numpy as np \n",
    "\n",
    "payload=\"Hello my name is Philipp. I am getting in touch with you because i didn't get a response from you. What do I need to do to get my new card which I have requested 2 weeks ago? Please help me and answer this email in the next 7 days. Best regards and have a nice weekend \"*2\n",
    "print(f'Payload sequence length: {len(tokenizer(payload)[\"input_ids\"])}')\n",
    "\n",
    "def measure_latency(pipe):\n",
    "    latencies = []\n",
    "    # warm up\n",
    "    for _ in range(10):\n",
    "        _ = pipe(payload)\n",
    "    # Timed run\n",
    "    for _ in range(300):\n",
    "        start_time = perf_counter()\n",
    "        _ =  pipe(payload)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    # Compute run statistics\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    time_p95_ms = 1000 * np.percentile(latencies,95)\n",
    "    return f\"P95 latency (ms) - {time_p95_ms}; Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f};\", time_p95_ms\n",
    "\n",
    "\n",
    "vanilla_model=measure_latency(vanilla_clf)\n",
    "quantized_model=measure_latency(quantized_clf)\n",
    "\n",
    "print(f\"Vanilla model: {vanilla_model[0]}\")\n",
    "print(f\"Quantized model: {quantized_model[0]}\")\n",
    "print(f\"Improvement through quantization: {round(vanilla_model[1]/quantized_model[1],2)}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc4c920-009f-451c-8455-cb7b96c21783",
   "metadata": {},
   "source": [
    "## Push quantized model to the Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79bfddd0-7133-404a-b15b-5b965476ecf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/hf/lib/python3.9/site-packages/huggingface_hub/hf_api.py:79: FutureWarning: `name` and `organization` input arguments are deprecated and will be removed in v0.8. Pass `repo_id` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "\n",
    "tmp_store_directory=\"onnx_hub_repo\"\n",
    "repository_id=\"quantized-distilbert-banking77\"\n",
    "model_file_name=\"model-quantized-dynamic.onnx\"\n",
    "\n",
    "model.latest_model_name=model_file_name # workaround for PR #214\n",
    "model.save_pretrained(tmp_store_directory)\n",
    "dynamic_quantizer.tokenizer.save_pretrained(tmp_store_directory)\n",
    "\n",
    "model.push_to_hub(tmp_store_directory,\n",
    "                  repository_id=repository_id,\n",
    "                  use_auth_token=True\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6697b734-fc70-4abb-916d-abee8c2e30ae",
   "metadata": {},
   "source": [
    "## Load and run inference from the Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ec42230-b021-40f4-9879-d377f3975e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ab146b4c61482391dace7322b71dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.78k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e2c4e6982b4223b9dc20a5f225682b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/141M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3356ff20b9849fb852c05a05feab137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/361 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8503ceec484b4417903c6a44eb5b8182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "debaeb2795af4285b4ee8555e5c4568e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/695k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd26b787bbe42bc8f6ea9c8f3ae4296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'cash_withdrawal_not_recognised', 'score': 0.07231996953487396}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "model = ORTModelForSequenceClassification.from_pretrained(\"lewtun/quantized-distilbert-banking77\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lewtun/quantized-distilbert-banking77\")\n",
    "\n",
    "remote_clx = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "remote_clx(\"What is the exchange rate like on this app?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2851e8c0-cb85-40a0-b216-e02d7f121b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
